# Gradient Descent
In the Perceptron Model, we explored using a predefined rule to improve our predictions. However, the gradient descent is a way to optimize our predictions (after all, efficiency is very important in coding!)

To better understand how gradient descent works, I will use a real world example. Because this whole Github repository is centered around wine, I will use the example of wine in a wine glass.

We have probably all seen a while glass: it is shaped like a parabola, with a minimum (or bottom). When we pour wine into the glass, where does the wine go? Does the wine stay on the edges of the wine glass? No, it automatically falls to the bottom of the glass. No matter how we pour the wine, it always ends up at the bottom. Now think of the wine glass like a cost (or error) function. We are trying to minimize the error as much as possible. Thus, we are trying to find the bottom of the wine glass where when we pour the wine, it falls to the bottom.
