# Random Forests
The idea behind Random Forests is very similar to the idea behind Bagging, but with a slight tweak. When implementing bagging, the models use the same set of features to decide where to break off the data. This means that while there may be difference in the results of the models, the models likely split the data at similar places. However, Random Forests adds yet another layer of randomness by having a random set of features instead of the full set of features. This yields more varied results, so that when aggregated, more accurate results are made. It is similar to the idea that if you have a group of people and given some attributes, they all tend to make the wrong assumption, then the data is still skewed. However, if you vary the attributes given, then the data becomes less skewed.
